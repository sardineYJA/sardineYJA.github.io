---
layout: post
title: "ES 知识点补充"
date: 2020-05-06
description: "Elasticsearch"
tag: Elasticsearch

---

## 安装searchguard关闭xpack部分功能

```sh
xpack.monitoring.enabled: true
xpack.security.enabled: false   
xpack.graph.enabled: false
xpack.ml.enabled: false
xpack.watcher.enabled: false
```

## Kibana

需要安装search-guard-kibana-plugin

```sh
server.host: XXX.XXX.XXX.XXX
elasticsearch.url: "http://XXX.XXX.XXX.XXX:9200"
elasticsearch.username: "admin"
elasticsearch.password: "admin"

xpack.monitoring.enabled: true
xpack.security.enabled: false        # xpack的 安全机制
xpack.reporting.enabled: false       # xpack的 报告机制
searchguard.session.keepalive: false
```

## 换GC

```sh
-XX:+UseG1GC
-XX:MaxGCPauseMillis=300
```



## 合并segment

```sh
GET _cat/segments/索引名?v
POST 索引名/_forcemerge?max_num_segments=1  # 合并成一个（根据具体情况）
GET _task?actions=*merge?detailed=true      

GET _cat/nodes?v    # ES节点上segment占用的内存
```


## 关闭索引

```sh
POST 索引名/_close
POST 索引名/_open
# 索引关闭，开销基本分0，但无法进行读写
```


## 删除.del文件

```sh
# es在检索过程中也会把.del文件进行检索过滤，如有大量的.del文件，应该让.del真正的从es中抹去，优化检索的效率。
# 在索引合并阶段就会删除.del文件
curl -X POST "http://node01:9200/索引名/_forcemerge?only_expunge_deletes=true"
```

## 查询特殊符号命名的index

转义字符用：%
例如：
```sh
# GET /<logstash-{now/d}>/_search
GET /%3Clogstash-%7Bnow%2Fd%7D%3E/_search
```

端口：lsof -i :5601   # 查看kibana进程


## Important Elasticsearch configuration

- path.data and path.logs
- cluster.name
- node.name
- bootstrap.memory_lock
- network.host
- discovery.zen.ping.unicast.hosts
- discovery.zen.minimum_master_nodes
- JVM heap dump path

## Important System Configuration

- Set JVM heap size
- Disable swapping
- Increase file descriptors
- Ensure sufficient virtual memory
- Ensure sufficient threads


## Task API

```sh
GET _tasks
GET _tasks?nodes=nodeId1,nodeId2&actions=cluster:*

GET _tasks?actions=*search&detailed=true
GET _tasks?actions=*update&detailed=true
GET _tasks?actions=*merge&detailed=true
GET _tasks?actions=*reindex&detailed=true

POST _tasks/node_id:task_id/_cancel   # 取消
POST _tasks/_cancel?nodes=nodeId1,nodeId2&actions=*reindex   # 取消多个任务
```

## 修改searchguard副本数
```sh
PUT searchguard/_settings
{
	"index": {
		"auto_expand_replicas": "0-3"
	}
}
```

## 防止脑裂导致版本差异
```sh
PUT _cluster/settings
{
	"transient": {
		"discovery": {
			"zen": {         # 一般3个master即可
				"minimun_master_nodes": "2"
			}
		}
	}
}
```

## 文档刷新时间
```sh
PUT */_settings
{
	"refresh_interval": -1
}
```

## Kibana 限制某些用户只允许查询某个特定字段值的doc

在Document Level Security Query添加条件

```sh
{
	"bool": {
		"must_not": [],
		"must": [
			{
				"terms": {
					# 只允许查询title为1的doc
					"title": ["1"]
				}
			}
		]
	}
}
```


## Reindex

yml配置：
```sh
reindex.remote.whitelist: "XXX.XXX.XXX.XXX:9200"
```

获取原模板：`GET _template`，修改refresh_interval为-1，number_of_replicas为0

```sh
POST _reindex
{

  "size": 5,
  "conflicts": "proceed",

  "source": {
    "remote": {
      "host": "http://oldhost:9200",
      "username": "user",
      "password": "pass"
    },

    "size": 2,      # 每批次量
    "index": "oldIndex",
    "type": "",
    "_source": ["name"],
    "query": {
      "match": {
        "test": "data"
      }
    }
  },

  "dest": {
    "index": "newIndex",       # 新索引
    "op_type": "create",       # 操作类型创建
    "routing": "=value"        # 以什么值为路由
  }
}
```

## rollover 自动限制索引大小

1. 手动创建索引，以及别名
```sh
# PUT /<logs-{now/d}-1>  例如：logs-2020.05.18-000001
PUT /%3Clogs-%7Bnow%2Fd%7D-000001%3E
{
  "aliases": {
    "logs_test_write": {}
  }
}
```

2. 修改logstash写入es的logs_test_write

3. 测试一下rollover

```sh
POST logs_test_write/_rollover
{
  "conditions": {
    "max_age": "7d",
    "max_docs": 10,
    "max_size": "5g"
  }
}
```
索引按照【”-”+数字】的形式结尾，新创建的索引数字+1，数字为6位，不够前面以0补齐。


4. 每次POST才会检查，所以需要写成cron定时任务
```sh
#!/bin/sh
curl -X POST "http://xxx.xxx.xxx.xxx/logs_test_write/_rollover" -H "Content-Type:application/json" -d '{"conditions": {"max_age": "1d", "max_docs": 10, "max_size": "100g"}}' 
```




# 问题

集群可能会有整体重启的需要，比如需要升级硬件、升级操作系统或者升级ES大版本。重启所有结点可能带来的一个问题: 某些结点可能先于其他结点加入集群。 先加入集群的结点可能已经可以选举好master，并立即启动了recovery的过程，由于这个时候整个集群数据还不完整，master会指示一些结点之间相互开始复制数据。 那些晚到的结点，一旦发现本地的数据已经被复制到其他结点，则直接删除掉本地“失效”的数据。 当整个集群恢复完毕后，数据分布不均衡显然是不均衡的，master会触发rebalance过程，将数据在结点之间挪动。整个过程无谓消耗了大量的网络流量。


## 单节点重启

升级版本或者故障，单个node关闭之前，关闭分片自动分配。
此时关闭节点，集群yellow。此节点primary在其他节点的replica被推荐为primary，自己变成replica。
由于设置为"none"，此节点replica不会在其他节点上复制恢复，保持unassigned状态。

重启后replica与primary存在差异时（即primary持续有新数据写入），需进入recovery过程。此过程需要主副之间拷贝数据，或者利用translog重放热数据。


一般关闭前：合并segment（refresh一次一个segment，尽可能少segment）；flush数据到磁盘；关闭自动分片。

```sh

PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.enable": "none"
		# 关闭分片自动分配，"all"为开启
	}
}

POST _flush/synced   # 手动同步，segment数据刷到磁盘

PUT */_settings
{
	"refresh_interval": -1     
}
```

启动后

```sh
PUT _cluster/settings
{
	"persistent": {
		"cluster.routing.allocation.enable": "all"
	}
}

# 此时集群有大量的 unassigned 分片，集群处于 yellow 状态。为了加快集群恢复的速度
PUT _cluster/settings
{
	"persistent": {
		# 调整分片恢复并发数，默认2
        "cluster.routing.allocation.node_concurrent_recoveries": 20,
        # 默认20mb，设置"none"为磁盘极限，提升recovery速度，减少数据写入被阻塞的时长
        "indices.recovery.max_bytes_per_sec": "40mb"
    }
}

##允许在一个节点上发生多少并发传入分片恢复。 默认为2
##多数为副本
"cluster.routing.allocation.node_concurrent_incoming_recoveries":2
##允许在一个节点上发生多少并发传出分片恢复，默认为2
## 多数为主分片
"cluster.routing.allocation.node_concurrent_outgoing_recoveries":2
##为上面两个的统一简写
"cluster.routing.allocation.node_concurrent_recoveries":2
```



## boost 提升权重

默认值 1

```sh
PUT my_index
{
  "mappings": {
    "_doc": {
      "properties": {
        "title": {
          "type": "text",
          "boost": 2 
        },
        "content": {
          "type": "text"
        }
      }
    }
  }
}
## 在查询时增强
POST _search
{
    "query": {
        "match" : {
            "title": {
                "query": "quick brown fox",
                "boost": 2
            }
        }
    }
}
```






## vim 删除空格：

删除空格行：`:g/^$/d`

删除行首空格：`:%s/^\s*//g`

删除行尾空格：`:%s/\s*$//g`


## 命令

`ps -ef | grep elasticsearch`

-e 所有用户所有进程 -f 详细信息，-a 是当前用户所有进程



# log4j2.properties

## 以 logstash 的日志配置为参考

默认是基于时间的触发策略，但不是每天一个log的意思，只有触发logstash-plain.log写操作（如重启，修改conf），才会进行滚动判断，如果几天没有写操作，可能就不会有那几天的log文件。

```sh
status = error
name = LogstashPropertiesConfig


# 控制台输出，logstash启动关闭会打印在console，与log文件内容一样

appender.console.type = Console
appender.console.name = plain_console
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n

appender.json_console.type = Console
appender.json_console.name = json_console
appender.json_console.layout.type = JSONLayout
appender.json_console.layout.compact = true
appender.json_console.layout.eventEol = true


## 滚动文件

appender.rolling.type = RollingFile
appender.rolling.name = plain_rolling
appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
appender.rolling.policies.type = Policies
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.rolling.policies.time.interval = 1
appender.rolling.policies.time.modulate = true
appender.rolling.layout.type = PatternLayout
appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %-.10000m%n


appender.json_rolling.type = RollingFile
appender.json_rolling.name = json_rolling
appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
appender.json_rolling.policies.type = Policies
appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.json_rolling.policies.time.interval = 1
appender.json_rolling.policies.time.modulate = true
appender.json_rolling.layout.type = JSONLayout
appender.json_rolling.layout.compact = true
appender.json_rolling.layout.eventEol = true

rootLogger.level = ${sys:ls.log.level}
rootLogger.appenderRef.console.ref = ${sys:ls.log.format}_console
rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
```

待测试...

```sh
# 新增
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.rolling.policies.size.size = 50MB
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.max = 30
```

```sh
# 基于时间的触发策略（TriggeringPolicy）
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
# filePattern中配置的文件重命名规则是test1-%d{yyyy-MM-dd-HH-mm}，
# 最小的时间粒度是mm，即分钟，TimeBasedTriggeringPolicy指定的size是1，结合起来就是每2分钟生成一个新文件。
# 如果改成%d{yyyy-MM-dd-HH}，最小粒度为小时，则每2个小时生成一个文件。
# %d{yyyy-MM-dd-HH}不能有空格，如%d{yyyy-MM-dd HH-mm}则不会滚动。
appender.rolling.policies.time.interval = 2
# 是否对保存时间进行限制。若modulate=true，则保存时间将以0点为边界进行偏移计算。
# 比如，modulate=true，interval=4hours，
# 那么假设上次保存日志的时间为03:00，则下次保存日志的时间为04:00，之后的保存时间依次为08:00，12:00，16:00
appender.rolling.policies.time.modulate = true

# 基于日志文件大小的触发策略
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.rolling.policies.size.size = 100MB

# 文件保存的覆盖策略，生成分割（保存）文件的个数
appender.rolling.strategy.type = DefaultRolloverStrategy
appender.rolling.strategy.max = 5
```



## pycharm 调试快捷键
```sh
Ctrl + Shift + F8    查找全部断点

F7                   单步进入

F8                   单步不进入

Shift + F8           跳出

F9                   下一个断点
```


## global 全局变量

global 全局变量，但仅限于在一个模块（py文件）中调用，通过定义全局变量模块，实现跨文件全局。
（main函数中声明的变量默认为global）

```python
# 定义全局模块，gol.py

def _init():#初始化
    global _global_dict
    _global_dict = {}
 
def set_value(key, value):
    _global_dict[key] = value
 
def get_value(key, defValue=None):
    try:
        return _global_dict[key]
    except KeyError:

```

```python
import gol
gol._init() # 先必须在Main主模块初始化
# 定义跨模块全局变量
gol.set_value('id',uuid)
```

```python
import gol
# 不需要再初始化了
id = gol.get_value('id')
```


## threading.Timer 定时任务

```python
import threading
import time

def sayHello(name):
    print ("hello %s\n", name) 
    global timer                                      # 定义全局，timer.cancel才能作用到此处
    timer = threading.Timer(2.0, sayHello, ["Meki"])  # 2秒
    timer.start()                                     # 非阻塞

if __name__ == "__main__":
    timer = threading.Timer(2.0, sayHello, ["Meki"])
    timer.start()                                     # 开启新线程

    time.sleep(10)
    timer.cancel()   # 取消
```

> start() 运行的就是 thread 的 run() 即开启新线程，而且都会 join (即main线程会等待所有线程结束才会结束)。cancel取消不会立刻中断定时任务，会等待此定时任务完成（但不会产生新Timer）。



## join 阻塞主线程

```python
# join之后,主线程被线程1阻塞,在线程1返回结果之前,主线程无法执行下一轮循环
threads = [Thread() for i in range(5)]
for thread in threads:
    thread.start()
    thread.join()

# 正确做法
threads = [Thread() for i in range(5)]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
```


## 集群内部 reindex
事先创建创建_template，并创建索引
```sh
# 数据量不大就无须此步骤
PUT */_settings
{
    "refresh_interval": "5s",   # 设置-1
    "number_of_replicas": 1     # 设置0
}
```

```sh
POST _reindex
{
  "conflicts": "proceed",
  "source": {
    "index": "oldIndex",
  },
  "dest": {
    "index": "newIndex", 
    "op_type": "create"  
  }
}

# 查看过程
cat _tasks/?pretty&detailed=true&actions=*reindex
```






