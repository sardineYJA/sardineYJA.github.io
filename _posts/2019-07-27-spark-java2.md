---
layout: post
title: "Spark java编程2"
date: 2019-07-27
description: "介绍一下Spark java编程2"
tag: 大数据

---

## RDD 通过 StructField 方法转 DataFrame 

```java
JavaRDD<Row> javaRdd = rdd.map(new Function<String, Row>() {
	@Override
	public Row call(String v1) throws Exception {
		String str[] = v1.split(":");

		if(str.length < 38) {
			return null;
		}
		String msg_id = str[1];
		String sp_number = str[5];
		String errorCode = str[35];
		return RowFactory.create(
				msg_id,
				sp_number,
				errorCode);
	}
});

import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
List<StructField> fields = new ArrayList<>();
fields.add(DataTypes.createStructField("msg_id", DataTypes.StringType, true));
fields.add(DataTypes.createStructField("sp_number", DataTypes.StringType, true));
fields.add(DataTypes.createStructField("errorCode", DataTypes.StringType, true));
StructType schema = DataTypes.createStructType(fields);
Dataset<Row> successRateDataDataset = sparkSession.createDataFrame(rdd, schema);
```

## 关于 SparkConf 参数

```java
SparkSession sparkSession = SparkSession.builder().appName("ServiceName")
		.config("spark.sql.warehouse.dir", "/user/sxdt/spark-warehouse")
		.config("spark.testing.memory", "2147480000")
		.master("local[*]")
		.enableHiveSupport()
		.getOrCreate();

		.config("es.index.auto.create", "true")
		.config("es.nodes", "183.238.157.218")
		.config("es.port", "42284")
		.config("es.nodes.wan.only", "true")
		.config("pushdown", "true")

SparkConf sparkConf = new SparkConf()
        .setAppName("SparkApplication")
        .setMaster("local")
        .set("spark.cores.max", "10");

// SparkSession.builder.config的配置对应SparkConf

// conf/spark-defaults.conf
spark.master                     spark://master:7077
spark.eventLog.enabled           true
spark.eventLog.dir               file:///home/yangja/sparklog
spark.eventLog.dir               hdfs://namenode:8021/directory
spark.eventLog.compress          true

spark.app.name, name        // AppName
spark.executor.memory, 1g   // 执行内存
spark.driver.cores, 1       // driver的cpu核数
spark.driver.memory, 512m   // driver的内存
spark.executor.memory, 512m // 每个executor内存

```


## 数据库操作



Dataset 保存/读取数据库

```java
Properties jdbcpro = new Properties();
jdbcpro.setProperty("user", PropertiesUtil.user);
jdbcpro.setProperty("password", PropertiesUtil.password);
jdbcpro.setProperty("driver", PropertiesUtil.driver);
String url = PropertiesUtil.url;
// 读取数据
Dataset<Row> dataset = sparkSession.sql(yourSQL);
// 数据库参数只需user,password,driver,url四个即可
dataset.write().mode(SaveMode.Append).jdbc(url, "table_name", jdbcpro); // 保存到数据库
sparkSession.read().jdbc(url, "yourSQL", jdbcpro).createOrReplaceTempView(tableName); // 读取
```


