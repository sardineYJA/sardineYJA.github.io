---
layout: post
title: "Spark java编程2"
date: 2019-07-27
description: "介绍一下Spark java编程2"
tag: 大数据

---

## 日志信息

slf4j(Simple Logging Facade for Java)，是一系列的日志接口，是Java中所有日志框架的抽象

创建src/main/resources/log4j.properties

```
log4j.rootCategory=INFO, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
```

```java
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
public static Logger logger = LoggerFactory.getLogger(Xxx.class);
logger.error("Error Message!");
logger.warn("Warn Message!");
logger.info("Info Message!");
logger.debug("Debug Message!");
```

## RDD 通过 StructField 方法转 DataFrame 

```java
JavaRDD<Row> javaRdd = rdd.map(new Function<String, Row>() {
	@Override
	public Row call(String v1) throws Exception {
		String str[] = v1.split(":");

		if(str.length < 38) {
			return null;
		}
		String msg_id = str[1];
		String sp_number = str[5];
		String errorCode = str[35];
		return RowFactory.create(
				msg_id,
				sp_number,
				errorCode);
	}
});

import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
List<StructField> fields = new ArrayList<>();
fields.add(DataTypes.createStructField("msg_id", DataTypes.StringType, true));
fields.add(DataTypes.createStructField("sp_number", DataTypes.StringType, true));
fields.add(DataTypes.createStructField("errorCode", DataTypes.StringType, true));
StructType schema = DataTypes.createStructType(fields);
Dataset<Row> successRateDataDataset = sparkSession.createDataFrame(rdd, schema);
```

## 关于 SparkConf 参数

```java
SparkSession sparkSession = SparkSession.builder().appName("ServiceName")
		.config("spark.sql.warehouse.dir", "/user/sxdt/spark-warehouse")
		.config("spark.testing.memory", "2147480000")
		.master("local[*]")
		.enableHiveSupport()
		.getOrCreate();

		.config("es.index.auto.create", "true")
		.config("es.nodes", "183.238.157.218")
		.config("es.port", "42284")
		.config("es.nodes.wan.only", "true")
		.config("pushdown", "true")

SparkConf sparkConf = new SparkConf()
        .setAppName("SparkApplication")
        .setMaster("local")
        .set("spark.cores.max", "10");

// SparkSession.builder.config的配置对应SparkConf

// conf/spark-defaults.conf
spark.master                     spark://master:7077
spark.eventLog.enabled           true
spark.eventLog.dir               file:///home/yangja/sparklog
spark.eventLog.dir               hdfs://namenode:8021/directory
spark.eventLog.compress          true

spark.app.name, name        // AppName
spark.executor.memory, 1g   // 执行内存
spark.driver.cores, 1       // driver的cpu核数
spark.driver.memory, 512m   // driver的内存
spark.executor.memory, 512m // 每个executor内存

```


## 数据库操作

db_server.propertie

```
driverClassName=oracle.jdbc.OracleDriver
url=jdbc:oracle:thin:@172.16.7.14:1521:ora10ha
username=root
password=123456
filters=stat
initialSize=2
maxActive=300
maxWait=60000
timeBetweenEvictionRunsMillis=60000
minEvictableIdleTimeMillis=300000
validationQuery=select 1 from dual
testWhileIdle=true
testOnBorrow=false
testOnReturn=false
poolPreparedStatements=false
maxPoolPreparedStatementPerConnectionSize=200
```

```java
Properties prop = new Properties();
prop.load(Xxx.class.getResourceAsStream("/db_server.properties"));
user = prop.getProperty("username");
password = prop.getProperty("password");
url = prop.getProperty("url");
driver = prop.getProperty("driverClassName");
```

Dataset 保存/读取数据库

```java
Properties jdbcpro = new Properties();
jdbcpro.setProperty("user", PropertiesUtil.user);
jdbcpro.setProperty("password", PropertiesUtil.password);
jdbcpro.setProperty("driver", PropertiesUtil.driver);
String url = PropertiesUtil.url;
// 读取数据
Dataset<Row> dataset = sparkSession.sql(yourSQL);
// 数据库参数只需user,password,driver,url四个即可
dataset.write().mode(SaveMode.Append).jdbc(url, "table_name", jdbcpro); // 保存到数据库
sparkSession.read().jdbc(url, "yourSQL", jdbcpro).createOrReplaceTempView(tableName); // 读取
```

java.sql 基础

```java
import java.sql.Driver;
import java.sql.Connection;
import java.sql.Statement;
import java.sql.PreparedStatement;
import java.sql.CallableStatement;
import java.sql.ResultSet;
import java.sql.DatabaseMetaData;
import java.sql.ResultSetMetaData;
```

```java
// Statement 用于执行不带参数的简单sql语句，executeQuery, executeUpdate, execute
// PreparedStatement(继承Statement) 执行带或不带参数的预编译sql语句
// CallableStatement(继承PreparedStatement) 调回数据库存储过程的方法
Class.forName("com.mysql.jdbc.Driver");
Connection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/name", "root", "123456");
Statement stmt = conn.createStatement();
ResultSet resultSet = stmt.executeQuery("yourSQL");
// prepareStatement对象防止sql注入的方式是把用户非法输入的单引号用\反斜杠做了转义
PreparedStatement preparedStatement = conn.prepareStatement("yourSQL");
ResultSet rs = preparedStatement.executeQuery();
```


## Druid 连接池


DRUID是阿里巴巴开源平台上一个数据库连接池实现，加入日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池


```xml
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>druid</artifactId>
    <version>1.1.9</version>
</dependency>
```

```java
import java.sql.*;
import java.util.Properties;
import com.alibaba.druid.pool.DruidDataSource;
import com.alibaba.druid.pool.DruidDataSourceFactory;
import com.alibaba.druid.pool.DruidPooledConnection;
```

```java
druidDataSource = new DruidDataSource();
druidDataSource.setDriverClassName("com.mysql.jdbc.Driver"); 
druidDataSource.setUsername("root");
druidDataSource.setPassword("123456");
druidDataSource.setUrl("jdbc:mysql://127.0.0.1:3306/demo"); 
druidDataSource.setInitialSize(5);
druidDataSource.setMinIdle(1);
druidDataSource.setMaxActive(10);
druidDataSource.setFilters("stat");
```

连接池实例

```java
public class DBPoolConnection {
	private static DBPoolConnection dbPoolConnection = null;
	private static DruidDataSource druidDataSource = null;
	// 初始化
	static {
		Properties properties = new Properties();
		// 读取数据库全部参数
		properties.load(DBPoolConnection.class.getResourceAsStream("db.properties"));
		druidDataSource = (DruidDataSource)DruidDataSourceFactory.createDataSource(properties);
	}
	// 连接池单例
	public static synchronized DBPoolConnection getInstance() {
		if (null == dbPoolConnection) {
			dbPoolConnection = new DBPoolConnection();
		}
		return dbPoolConnection;
	}
	// 返回druid数据库连接
	public DruidPooledConnection getConnection() {
		return druidDataSource.getConnection();
	}
	// 释放资源
	public static void closeAll(DruidPooledConnection conn, PreparedStatement ps, ResultSet rs) {
		if (rs != null) {
			rs.close();
		}
		if (ps != null) {
			ps.close();
		}
		if (conn != null) {
			conn.close();
		}
	}
}
```

```java
DBPoolConnection dbp = DBPoolConnection.getInstance();
DruidPooledConnection conn = null;
PreparedStatement ps = null;
ResultSet rs = null;

conn = dbp.getConnection();
String sql = "UPDATE table_name SET name=? where age=?;"
ps = conn.prepareStatement(sql);
ps.setString(1, "yang");
ps.setString(2. "18");
ps.executeUpdate();
DBPoolConnection.closeAll(conn, ps, rs);
```

获取当前时间

```java
import java.text.SimpleDateFormat;
import java.util.Date;
SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
System.out.println(df.format(new Date()));
// 2019-08-19 17:50:46

import java.sql.Timestamp;
Timestamp currentTime = new Timestamp(System.currentTimeMillis());
System.out.println(currentTime);
// 2019-08-19 17:50:46.452
```