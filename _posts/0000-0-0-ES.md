---
layout: post
title: "ES 全文检索"
date: 2019-07-06
description: "Elasticsearch"
tag: Elasticsearch

---

# ES 版本7的改变

- 从7.0开始默认安装了java运行环境，future versions of Elasticsearch will require Java 11，但是还是可以启动成功，因为向下兼容

- Type 被舍弃，一个Index只能创建一个Type：`_doc`

- 新建 Index，默认主分片 Share 为 1

- 彻底废弃`_all`字段支持，为提升性能默认不再支持全文检索，即7.0之后版本进行该项配置会报错。

- 7.0将不会再有OOM的情况，JVM引入了新的circuit breaker（熔断）机制，当查询或聚合的数据量超出单机处理的最大内存限制时会被截断，并抛出异常



# Elasticsearch

Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。

## 全文检索

程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的`次数`和`位置`。
当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。




## 概念

- Index 索引（相当于数据库）：包含一堆有相似结构的文档数据

- Type 类型（相当于表）：每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field（相当于一列）

- Document 文档（相当于行）：文档是es中的最小数据单元，一个document可以是一条客户数据

- Field 字段（列）：Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段。

- Inverted Index （倒排索引）: 每一个文档都对应一个ID。倒排索引会按照指定语法对每一个文档进行分词，然后维护一张表，列举所有文档中出现的terms以及它们出现的文档ID和出现频率。搜索时同样会对关键词进行同样的分词分析，然后查表得到结果。


## Shard (分片) 

- 多分片好处：一个索引如果分布在多个节点，则可以并行执行。

- 每个分片是一个Lucene的索引，会使用资源。过多的分片分导致额外性能开销。

- 分片类型：Primary Shared & Replica Shared 。

- 每一个分片还会进一步拆分为分段（Segment）。

- 一个索引中的数据保存在多个分片中，相当于水平分表。一个分片便是一个Lucene 的实例，它本身就是一个完整的搜索引擎。文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。

- ES实际上就是利用分片来实现分布式。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。当你的集群规模扩大或者缩小时， ES会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。

- 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。

- 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。默认情况下，一个索引会有5个主分片，而其副本可以有任意数量。

- 主分片和副本分片的状态决定了集群的健康状态。每一个节点上都只会保存主分片或者其对应的一个副本分片，相同的副本分片不会存在于同一个节点中。如果集群中只有一个节点，则副本分片将不会被分配，此时集群健康状态为yellow，存在丢失数据的风险。



## 节点配置


（ES节点默认）：既有成为主节点的资格，又可以存储数据，还可以作为预处理节点。
```sh
node.master: true 
node.data: true 
node.ingest: true
```

（master节点）：有成为主节点的资格，可以参与选举。生产环境尽量分离主节点和数据节点，创建独立主节点。
```sh
node.master: true 
node.data: false 
node.ingest: false
```

（data节点）数据节点。
```sh
node.master: false 
node.data: true 
node.ingest: false
```

（Ingest节点）预处理节点。
```sh
node.master: false 
node.data: false 
node.ingest: true
```

（Coordinating node）协调节点。
```sh
node.master: false 
node.data: false 
node.ingest: false
```

半数选举：`discovery.zen.minimum_master_nodes`属性设置 NodesNum/2 + 1。

每个节点都保存了集群的状态，只有Master节点才能修改集群的状态信息。


## Master 选举

Elasticsearch的选主是ZenDiscovery模块负责的，主要包含Ping（节点之间通过这个RPC来发现彼此）和Unicast（单播模块包含一个主机列表以控制哪些节点需要ping通）这两部分；
对所有可以成为master的节点（node.master: true）根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。
如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。



# Write 操作

## 索引新文档（create）

当用户向一个节点提交了一个索引新文档的请求，节点会计算新文档应该加入到哪个分片（shard）中。每个节点都存储有每个分片存储在哪个节点的信息，因此协调节点会将请求发送给对应的节点。注意这个请求会发送给主分片，等主分片完成索引，会并行将请求发送到其所有副本分片，保证每个分片都持有最新数据。

每次写入新文档时，都会先写入内存中，并将这一操作写入一个translog文件（transaction log）中，此时如果执行搜索操作，这个新文档还不能被索引到。

![png](/images/posts/all/ES写操作图1.png)


ES会每隔1秒时间（这个时间可以修改）进行一次刷新操作（refresh），此时在这1秒时间内写入内存的新文档都会被写入一个文件系统缓存（filesystem cache）中，并构成一个分段（segment）。此时这个segment里的文档可以被搜索到，但是尚未写入硬盘，即如果此时发生断电，则这些文档可能会丢失。

![png](/images/posts/all/ES写操作图2.png)


不断有新的文档写入，则这一过程将不断重复执行。每隔一秒将生成一个新的segment，而translog文件将越来越大。

![png](/images/posts/all/ES写操作图3.png)


每隔30分钟或者translog文件变得很大，则执行一次fsync操作。此时所有在文件系统缓存中的segment将被写入磁盘，而translog将被删除（此后会生成新的translog）。

![png](/images/posts/all/ES写操作图4.png)


由上面的流程可以看出，在两次fsync操作之间，存储在内存和文件系统缓存中的文档是不安全的，一旦出现断电这些文档就会丢失。所以ES引入了translog来记录两次fsync之间所有的操作，这样机器从故障中恢复或者重新启动，ES便可以根据translog进行还原。

当然，translog本身也是文件，存在于内存当中，如果发生断电一样会丢失。因此，ES会在每隔5秒时间或是一次写入请求完成后将translog写入磁盘。可以认为一个对文档的操作一旦写入磁盘便是安全的可以复原的，因此只有在当前操作记录被写入磁盘，ES才会将操作成功的结果返回发送此操作请求的客户端。


此外，由于每一秒就会生成一个新的segment，很快将会有大量的segment。对于一个分片进行查询请求，将会轮流查询分片中的所有segment，这将降低搜索的效率。因此ES会自动启动合并segment的工作，将一部分相似大小的segment合并成一个新的大segment。合并的过程实际上是创建了一个新的segment，当新segment被写入磁盘，所有被合并的旧segment被清除。

![png](/images/posts/all/ES写操作图5.png)

![png](/images/posts/all/ES写操作图6.png)


```sh
数据 ---> 内存 ---> filesystem cache 形成 segment
操作 ------> translog ------> segment 写进磁盘，translog 清空  
```

## 更新（Update）和删除（Delete）文档

ES的索引是不能修改的，因此更新和删除操作并不是直接在原索引上直接执行。

每一个磁盘上的segment都会维护一个del文件，用来记录被删除的文件。每当用户提出一个删除请求，文档并没有被真正删除，索引也没有发生改变，而是在del文件中标记该文档已被删除。因此，被删除的文档依然可以被检索到，只是在返回检索结果时被过滤掉了。每次在启动segment合并工作时，那些被标记为删除的文档才会被真正删除。

更新文档会首先查找原文档，得到该文档的版本号。然后将修改后的文档写入内存，此过程与写入一个新文档相同。同时，旧版本文档被标记为删除，同理，该文档可以被搜索到，只是最终被过滤掉。



# Read 操作

分为两个阶段：
- 查询（query）
- 取回（fetch）

## 查询阶段

当一个节点接收到一个搜索请求，则这个节点就变成了协调节点。

![png](/images/posts/all/ES读操作图1.png)

第一步是广播请求到索引中每一个节点的分片拷贝。 查询请求可以被某个主分片或某个副本分片处理，协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。

每个分片将会在本地构建一个优先级队列。如果客户端要求返回结果排序中从第from名开始的数量为size的结果集，则每个节点都需要生成一个from+size大小的结果集，因此优先级队列的大小也是from+size。分片仅会返回一个轻量级的结果给协调节点，包含结果集中的每一个文档的ID和进行排序所需要的信息。

协调节点会将所有分片的结果汇总，并进行全局排序，得到最终的查询排序结果。此时查询阶段结束。

最终协调节点需要处理：`number_of_shard * (from + size)`，产生深度分页的性能问题。


## 取回阶段

查询过程得到的是一个排序结果，标记出哪些文档是符合搜索要求的，此时仍然需要获取这些文档返回客户端。

协调节点会确定实际需要返回的文档，并向含有该文档的分片发送get请求；分片获取文档返回给协调节点；协调节点将结果返回给客户端。

![png](/images/posts/all/ES读操作图2.png)



# 相关性计算

相关性算分问题：
每个分片都是基于自己的分片上的数据进行相关度计算。
所以数据量不大的时候，将主分片设置为1，防止算分偏离。


## TF-IDF

![png](/images/posts/all/Lucene中的TF-IDF评分公式.PNG)

即越罕见词匹配上，文档得分更高。


## BM25

从ES5.0开始，将默认算法改为BM25。

![png](/images/posts/all/ES相关性算分BM25算法公式.PNG)

主要解决：当TF无限增加时，BM25算分会趋向于一个稳定数值。



# 配置

## failure detection (故障检测)

管理节点会发生Ping请求至其他节点，并等待回应。

elasticsearch.yml配置ping，设置节点与节点之间的连接ping时长：
```
discovery.zen.ping.timeout=3s
```


## jvm.options

- 以`-开头`的行是不依赖JVM版本的选项
- 以`数值:-开头`的行是依赖于JVM版本的选项
- 以`数值-:-开头`的行是大于等于JVM版本的选项
- 以`数值-数值:-开头`的行是在这个JVM版本之间的选项

生产环境需要机器一半内存较好，另一半给page cache
```sh
-Xms1g
-Xmx1g
```

jmap -heap 查看jvm实际为ES进程分配的新生代的大小
```
## 即Survivor区对象经历15次Young GC后才进入老年代
-XX:MaxTenuringThreshold=15
```

GC 配置，使用CMS并发收集，老年代使用到75%就回收
```
8-13:-XX:+UseConcMarkSweepGC
8-13:-XX:CMSInitiatingOccupancyFraction=75
8-13:-XX:+UseCMSInitiatingOccupancyOnly
```
java 14 默认换成换成 G1



## 常用字段

- keyword类型：支持精确匹配，支持聚合、排序操作。适合精准字段匹配，keyword支持的最大长度为32766个UTF-8字符。如：url、name、title等字段。

- text类型：支持分词、全文检索，不支持聚合、排序操作。适合大字段存储，text对字符长度没有限制。如：文章详情、content字段等；

- 数值类型：short, integer, long, float, double

- date类型

- boolean类型





# reference

https://blog.csdn.net/zkyfcx/article/details/79998197

https://www.jianshu.com/p/fa510352ce1a



