---
layout: post
title: "ES 关于 JVM 的优化"
date: 2020-01-08
description: "ES 关于 JVM 的优化"
tag: ELK

---

## ES 中 JVM 性能调优

- ES设置JVM的配置文件：`.../config/jvm.options`

- 启动时设置：`ES_JAVA_OPTS="-Xms2g -Xmx2g" ./bin/elasticsearch`

- 操作系统配置参数：page cache 和文件描述符的个数：`/etc/security/limits.conf`

- 将Xms和Xmx设置成一样大（默认1G），避免JVM堆的动态调整给应用进程带来不稳定。为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源，可以减轻伸缩堆大小带来的压力。

- 预留足够的内存空间给page cache（机器内存一半）。

- Xmx不要超过机器内存的50%，也不要超过30G。（参考链接）

- 禁用内存交换(/proc/sys/vm/swappiness)

- -XX:+AlwaysPreTouch 减少新生代晋升到老年代时停顿。在启动时就把参数里说好了的内存全部都分配了，会使得启动慢上一点，但后面访问时会更流畅，比如页面会连续分配，比如不会在晋升新生代到老生代时才去访问页面，导致GC停顿时间加长。

- 生产环境中，JVM 必须使用 Server 模式


## 内存

Lucene的设计目的是把底层OS里的数据缓存到内存中。Lucene的段是分别存储到单个文件中的，这些文件都是不会变化的，所以很利于缓存，同时操作系统也会把这些段文件缓存起来，以便更快的访问。最后标准的建议是把50%的内存给elasticsearch，剩下的50%也不会没有用处的，Lucene会很快吞噬剩下的这部分内存用于文件缓存。


Xmx不要超过32G，jvm在内存小于32G的时候会采用一个内存对象指针压缩技术。超过32G后，每个对象的指针都变长了，就会使用更多的CPU内存带宽。

## page cache

swapping内存交换到磁盘对服务器性能来说是致命的。

- 暂时禁用：`swapoff -a`。
- 查看：`sysctl -q vm.swappiness`。
- 修改/etc/sysctl.conf，加入vm.swappiness=xxx 或/proc/sys/vm/swappiness文件。
- 重起系统或通过sysctl -p动态加载/etc/sysctl.conf文件。

page cache的清除策略是LRU(最近最少使用 Least Recently Used)

当Linux系统内存不足时，要么就回收page cache，要么就内存交换(swap)，而内存交换就有可能把应用程序的数据换出到磁盘上去了，这就有可能造成应用的长时间停顿了。

swappiness设置成0，表示优先刷新page cache而不是将应用程序的内存交换出去（到磁盘上）。

当内存不足时，有很大机率不是把用作IO缓存的Page Cache收回，而是把冷的应用内存page out到磁盘上。当这段内存重新要被访问时，再把它重新page in回内存（所谓的主缺页错误），这个过程进程是停顿的。增长缓慢的老生代，池化的堆外内存，都可能被认为是冷内存。


## JVM长时间停顿的示例

随着系统的运行，JVM堆内存会被使用，引用不可达对象就是垃圾对象，而操作系统有可能会把这些垃圾对象（长期未用的物理页面）交换到磁盘上去。
当JVM堆使用到一定程度时，触发FullGC，FullGC过程中发现这些未引用的对象都被交换到磁盘上去了，于是JVM垃圾回收进程得把它们重新读回来到内存中，然而戏剧性的是：这些未引用的对象本身就是垃圾，读到内存中的目的是回收被丢弃它们！而这种来回读取磁盘的操作导致了FullGC 消耗了大量时间，(有可能)发生长时间的stop the world（STW全局暂停）。
因此，需要禁用内存交换以避免这种现象，这也是为什么在部署Kafka和ES的机器上都应该要禁用内存交换的原因吧(如果因长时间STW导致node与master之间的"心跳包"失效，集群的master节点认为该节点发生了故障，于是是自动进行failover，对于ES来说可能就发生自动rebalance分片迁移)。


# reference

https://www.elastic.co/cn/blog/a-heap-of-trouble

https://www.cnblogs.com/hapjin/p/11135187.html

https://blog.csdn.net/wangmaohong0717/article/details/76218337




