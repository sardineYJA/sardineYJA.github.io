

# 聊天

2019.6 实习裸辞，找了一个月，一度想换到C++开发。

2020.2 裸辞，心情焦虑复习不到位，面试某些公司回答不好，谈薪酬不敢提高。

总结：不要裸辞，2020.4.22 记录并提醒自己



## 离职原因

大数据项目过少，基本没什么项目，想要进一步发展，希望可以去更优秀的公司进一步发展


## 想问的问题

我想知道这个职位的具体职责是做什么的，希望我可以做什么。

说项目，要说多一点，详细一点，



## 自我介绍

2018.7-2019.6 实习数据开发

2019.6-2020.1 大数据开发



# 工作经历

## 深圳市北科瑞声科技股份有限公司   ——   数据开发实习生     2018年7月 —— 2019年6月     
公司主要产品有智能会议系统、深眼系统、交通智能客服机器人。
本人主要工作职责：开发Python爬虫，文本数据的爬取和整理过滤，文本分类测试，Elasticsearch问答系统维护。

## 深圳市深讯数据科技股份有限公司   ——   大数据开发         2019年7月 —— 2020年1月
公司主要服务于电信集团的行业网关，开发和维护日志统计分析平台，开发政府部门网络平台。
本人主要工作职责：搭建大数据测试环境，编写Spark Core/SQL数据开发代码，对电信行业网关数据进行统计，搭建Elasticsearch检索平台，维护部分C++模块。



# 实习项目


## 推荐商品

（科研项目）基于商品评论属性的新型商品配置器及电商平台应用研究
项目内容：通过对商品评论文本分类、商品情感标签挖掘以及用户搜索的需求识别三方面研究，完成用户需求文本到商品真实评价标签的映射，为用户节省了浏览评论时间的同时实现高质量、个性化的商品推荐。

主要负责：开发Python爬虫进行电商数据的爬取，使用机器学习方法和scikit-learn库进行文本分类以及关键词提取。


## 客服机器人

交通咨询智能客服机器人项目
项目内容：基于Rasa框架的任务导向问答系统，结合意图识别技术、命名实体识别技术以及对话管理技术开发粤通卡交通咨询智能客户机器人，通过问答的形式向机器人获取关于粤通卡的信息。

主要负责：使用Python+BeautifulSoup进行智能爬虫的开发，使用itchat库进行问答数据挖掘。搭建Elasticsearch快速检索引擎，问答数据的导入维护。

解析网页：xpath, beautifulSoup, 正则表达式re
反爬机制：user-agent用户代理（Headers头部修改），IP地址代理池，时间停顿
scrapy爬虫框架， scrapy-redis分布式爬虫
机器学习的方法：Kmean，KNN，SVM，BERT，fasttext，NER
基本上调参比较多：词向量维度，训练次数，模型的参数
难点重点：数据的处理，参数调优




# 大数据项目


## 关于数据

组件版本：
- Hadoop 2.7
- Spark 2.1.1
- Elasticsearch 7.4.1
- Hbase 1.3.2
- Hive 2.1.1
- Kafka_2.11-0.11


集群规模：5台 64G 1T Centos7

参数优化：
```sh
spark.storage.memoryFraction
spark.shuffle.memoryFraction
spark.locality.wait = 0s
spark.serializer = kryo
spark.speculation true
driver内存，executor内存
```

数据倾斜：
```
Spark：过滤部分key；将大量key加随机前缀，先局部聚合，再全局聚合；广播变量；MapJoin；repartition；函数设置并行度numTask

Hadoop：Inputformat合并小文件；JVM重用；增大环形缓冲区；Combiner减少IO；Snappy压缩；使用SequenceFile

Hive：拉取more模式；开启本地MR；开启map端聚合；开启数据倾斜时负载均衡；开启严格模式；JVM重用；开启推测机制

HBase：HA搭建；rowkey设计散列；开启布隆过滤；region预分区
```




## 日志分析平台

项目名称：北京电信行业网关日志分析平台
开发环境：IDEA + JDK + maven
软件架构：Hadoop+Zookeeper+Spark+Mysql+Elasticsearch

项目描述：行业网关日志分析平台是为北京电信内部做的一款数据分析产品，北京行业网关日志由电信内部出原始数据，以FTP形式传递到服务器，将数据进行清洗过滤并上传到hdfs，Spark进行特定字段统计分析，将其结果入库。Elasticsearch作为搜索引擎，提供对需求快速检索功能。

主要职责：
- 参与架构的选型及业务数据处理流程，各指标的业务逻辑算法文档讨论
- 搭建大数据集群测试环境，编写shell脚本(集群启动关闭、数据上传的hdfs)
- 使用Java编写和调试Spark Core/SQL对数据统计分析和入库的程序代码
- 利用Elasticsearch开发快速检索需求的接口代码，并优化相关性能

## 日志分析详细内容

数据量：
一天5个G，MT和MO详单，2千万条
保留半年，1T，副本2，共3T
15分钟计算一次结果的需求，所以存到hdfs上的文件需要15分钟一个目录来控制



## 智慧旅游平台

项目名称：大数据智慧旅游平台

开发环境：IDEA + JDK + maven

软件架构：Hadoop+Spark+Kafka+Zookeeper+Hive+HBase+Mysql

项目描述：     
智慧旅游是为山东移动内部做的一款旅游行业的数据分析的产品，专门针对山东省内旅游局各景区的数据进行分析，通过该平台数据分析报告，能快速了解各景点景区概况(如：拥挤程度、游客的兴趣爱好、到访景区的交通方式等)，以便景点根据分析报告，对不足之处的改进，能更好的吸引更多的游客来观景。

主要职责：

- 参与架构的选型及业务数据处理流程，各指标的业务逻辑算法文档讨论
- 搭建大数据集群测试环境
- 编写sparkstreaming接收kafka数据并清洗整理上传到hdfs代码
- 取hdfs上前一天的数据并过滤常驻/过路人口之后，hive统计数据字段信息


## 智慧旅游详细内容

项目实现：
1. 各景区的数据由山东移动内部收集出最原始的数据，并以流式形式发到kafka中，用sparkstreaming接收kafka中的数据，将接收到的数据进行清洗，过滤不符合规则的数据，并将清洗后的结果以追加形式写入到hdfs中
2. 由于该项目有15分钟计算一次结果的需求，所以存到hdfs上的文件需要15分钟一个目录来控制
3. 各景区基础数据处理，将该数据存入Hive的一个中间结果表A中，A表为按每月每日分区表，基础数据处理为当天处理昨天的数据，每天处理一次
4. 取hdfs上昨天的数据，过滤常驻人口、过路人口之后，并获取后续统计需要的字段信息，将这些处理的之后的结果以每天追加的形式写入到Hive的A表中

（常驻人口定义游客30天有N天在景区出现过算常驻，过路人口停留时间短暂）
 
统计系统的各需求指标：
年龄、性别、消费、归属地、app使用排名、到访景区交通方式、过夜人数及天数、停留时间、首访景区、到访频次、景区轨迹、手机品牌、职业、搜索关键词；
离线统计，每天从A表中获取前一天的数据统计，每天处理一次
将各指标处理后的结果，以追加形式写入到mysql的结果表中

系统实时需求指标：
实时客流、实时客源、实时基站定义15分钟统计一次结果，即伪实时，需要每15分钟的数据进行统计计算
实时客流需求定义：游客从入园开始直到离开，这段时间一直视为该景区的实时客流，即需要将各游客每15分钟明细，按景区分类写入到Hbase中,下次实时统计是需要关联hbase表来使用
实时指标需求，每15分钟执行一次




## 丁鸿(没修过版)

l  大数据项目部分：大数据智慧旅游平台
Ø  开发环境：IDEA + JDK + maven
Ø  软件架构：hadoop+spark+kafka+zookeeper+hive+hbase+mysql
Ø  项目描述：     
智慧旅游是为山东移动内部做的一款旅游行业的数据分析的产品，专门针对山东省内旅游局各景区的数据进行分析，通过该平台数据分析报告，能快速了解各景点景区概况(如：拥挤程度、游客的兴趣爱好、到访景区的交通方式等)，以便景点根据分析报告，对不足之处的改进，能更好的吸引更多的游客来观景
Ø  项目实现：
1、 数据采集à清洗、合并à上传HDFS集群
a)     各景区的数据由山东移动内部收集出最原始的数据，并以流式形式发到kafka中
b)     用sparkstreaming接收kafka中的数据，将接收到的数据进行清洗，过滤不符合规则的数据，并将清洗后的结果以追加形式写入到hdfs中
c)      由于该项目有15分钟计算一次结果的需求，所以存到hdfs上的文件需要15分钟一个目录来控制
2、 各景区基础数据处理，将该数据存入Hive的一个中间结果表A中，A表为按每月每日分区表，基础数据处理为当天处理昨天的数据，每天处理一次
a)     取hdfs上昨天的数据，过滤常驻人口、过路人口之后，并获取后续统计需要的字段信息，将这些处理的之后的结果以每天追加的形式写入到Hive的A表中
b)     常驻人口定义游客30天有N(可配置)天在景区出现过算常驻
c)      过路人口根据各景区配置参数指定
 
3、 统计系统的各需求指标
a)     年龄、性别、消费、归属地、app使用排名、到访景区交通方式、过夜人数及天数、停留时间、首访景区、到访频次、景区轨迹、手机品牌、职业、搜索关键词；
b)     离线统计，每天从A表中获取前一天的数据统计，每天处理一次
c)      将各指标处理后的结果，以追加形式写入到mysql的结果表中
4、 系统实时需求指标
a)     实时客流、实时客源、实时基站定义15分钟统计一次结果，即伪实时，需要每15分钟的数据进行统计计算
b)     实时客流需求定义：游客从入园开始直到离开，这段时间一直视为该景区的实时客流，即需要将各游客每15分钟明细，按景区分类写入到Hbase中,下次实时统计是需要关联hbase表来使用
c)      实时指标需求，每15分钟执行一次
5、 省份城市级维度的基础数据处理及即后续统计的指标，业务逻辑跟上述一样
Ø  责任描述
1、 参与架构的选型及业务数据处理流程，并给出建议
2、 负责整理项目需求指标的定义，并编写各指标的业务逻辑算法文档
3、 负责数据收集整理、基础数据入库这块程序代码的编写
4、 负责整个项目的部署、上线调试等工作
